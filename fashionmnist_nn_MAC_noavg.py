# -*- coding: utf-8 -*-
"""Copia di AleLoVerde_HandsOnLab_FashionMNIST_NN_COLAB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13CjXcOWA7IXisRDl_hybsKjFZga46CJ6

**Setting Up Libraries**

This section includes the necessary library imports and setup for data processing and visualization.

It imports libraries such as PyTorch, torchvision, Matplotlib for plotting, time for timing operations, and more.
"""

# Import necessary libraries
import torch
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.dataset import TensorDataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
import time
import torch.nn.functional as F
import pandas as pd
import seaborn as sns
from collections import Counter
import numpy as np

"""**Custom Dataset Class for Data Handling**

This code defines a custom dataset class named imageDataset. The class is designed to facilitate the organization and management of data for image classification.

It includes the following methods:

- __init__(self, X_data, y_data): The constructor initializes the dataset with input data (e.g., images) stored in X_data and their corresponding target labels in y_data.

- __getitem__(self, index): This method allows you to retrieve a specific item from the dataset by providing an index. It returns a tuple containing the input data and its associated target label at the specified index.

- __len__(self): This method returns the total number of items in the dataset, which is determined by the length of X_data.
"""

class imageDataset(Dataset):
    # Constructor to initialize the dataset with data and labels
    def __init__(self, X_data, y_data):
        self.X_data = X_data  # Input data (e.g., images)
        self.y_data = y_data  # Target labels

    # Method to retrieve an item from the dataset at a given index
    def __getitem__(self, index):
        return self.X_data[index], self.y_data[index]

    # Method to return the total number of items in the dataset
    def __len__(self):
        return len(self.X_data)

"""**Data and Data Loading**

- **Data Loading**: This code loads both training and test data for the FashionMNIST dataset.
Training_data and test_data are instances of torch.utils.data.Dataset

-  **NumPy Conversion**: The loaded data is converted from PyTorch tensors to NumPy arrays for further processing.

-  **Tensor Conversion**: It transforms the NumPy arrays into PyTorch tensors.

-  **Label Frequencies**: The code counts and records the frequency of each label (class) in the training data using the Counter function from the collections library. This information helps in understanding the label distribution in the dataset. In this case all the labels contain the same number of images.

"""

# Load the training data
training_data = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor()
)

# Convert training data to NumPy arrays
x_train = training_data.data.numpy()
y_train = training_data.targets.numpy().astype(np.uint8)

# Load the test data
test_data = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor()
)

# Convert test data to NumPy arrays
x_test = test_data.data.numpy()
y_test = test_data.targets.numpy().astype(np.uint8)

# Convert NumPy arrays to PyTorch Tensors
x_train_tens, x_test_tens = map(torch.FloatTensor, (x_train, x_test))
y_train_tens, y_test_tens = map(torch.LongTensor, (y_train, y_test))

# Print the first item in the test data (a tensor)
# print(x_test_tens[0])

# Calculate label frequencies using Counter
frequency_counter = Counter(y_train)

# Print the label frequencies
print(frequency_counter)

"""**Data visualization**

This code uses Matplotlib to display a 3x3 grid of sample images from the FashionMNIST dataset. It randomly selects images and their corresponding labels from the training data and shows them with their class labels.

"""

# Create a figure for displaying images
figure = plt.figure(figsize=(8, 8))
cols, rows = 3, 3

# Define class labels for display
labels_names = ["T-shirt", "Trousers", "Pull-over", "Dress", "Jacket", "Sandal", "Shirt", "Sneakers", "Bag", "Boots"]

# Display a grid of sample images
for i in range(1, cols * rows + 1):
    # Randomly select an index from the training data
    sample_idx = torch.randint(len(x_train), size=(1,)).item()
    img = x_train[sample_idx]
    label = y_train[sample_idx]

    # Crop the image to the top 20 rows
    img = img[:28, :]

    # Add a subplot to the figure
    figure.add_subplot(rows, cols, i)
    plt.title(labels_names[label])  # Display the class label
    plt.axis("off")  # Turn off axis labels
    plt.imshow(img.squeeze(), cmap="gray")  # Display the image in grayscale

# Show the figure with the sample images
plt.show()

#img.shape

"""**Data Loading**

This code prepares the data for training and testing by reshaping it and creating datasets and dataloaders.  


"""

# Record the current time
tic = time.time()

def create_data_loaders(x_train_tensor, y_train_tensor, x_test_tensor, y_test_tensor, batch_size = 64, shuffle=True):
    # Reshape training data and create a training dataset and dataloader
    train_ds = imageDataset(x_train_tensor, y_train_tensor)
    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)

    # Reshape test data and create a test dataset and dataloader
    test_ds = imageDataset(x_test_tensor, y_test_tensor)
    test_dl = DataLoader(test_ds, batch_size=batch_size)

    return train_ds, train_dl, test_ds, test_dl

# Set the batch size
batch_size = 64

# Reshape training data and create a training and test dataset and dataloader
x_train_tensor = x_train_tens.reshape([-1, 784])
x_test_tensor = x_test_tens.reshape([-1, 784])
train_ds, train_dl, test_ds, test_dl = create_data_loaders(x_train_tensor, y_train_tens, x_test_tensor, y_test_tens, batch_size = batch_size)

# Print the shapes of the first batch of data and labels
for x, y in train_dl:
   print(x.shape, y.shape)
   break


# Calculate and print the elapsed time
f'{time.time() - tic:.2f} sec'

#for batch in train_dl:
#  input, targets = batch
#  print(batch[1].__getitem__(1))
#print(train_ds.X_data.shape)
#print(train_dl[0,1])

"""**Neural Network Class**

This code defines a feed-forward neural network with multiple layers. The network consists of four fully connected layers (also known as linear layers). Each hidden layer uses the Rectified Linear Unit (ReLU) activation function, introducing non-linearity to the network. The final layer is the output layer with ten units, same number as the output clases to be classified.

"""

# Define a feed-forward neural network with multiple layers
class NeuralNetwork(torch.nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()

        # Define the layers
        self.fc1 = torch.nn.Linear(784, 512)  # Fully connected layer 1
        self.fc2 = torch.nn.Linear(512, 256)  # Fully connected layer 2
        self.fc3 = torch.nn.Linear(256, 128)  # Fully connected layer 3
        self.fc4 = torch.nn.Linear(128, 10)   # Fully connected layer 4 (output layer)

    def forward(self, x):
        # Define the forward pass
        x = F.relu(self.fc1(x))  # Apply ReLU activation to the first hidden layer
        x = F.relu(self.fc2(x))  # Apply ReLU activation to the second hidden layer
        x = F.relu(self.fc3(x))  # Apply ReLU activation to the third hidden layer
        x = self.fc4(x)         # Final output layer

        return x

"""**Print Network architecture**

The print(model) statement will display the architecture of the neural network. model.state_dict() includes the learnable parameters of the model's layers.

"""

model = NeuralNetwork()  # Create an instance of the NeuralNetwork
print(model)  # Display the model architecture

# To view the model's state dictionary
# state_dict = model.state_dict()
# print(state_dict)

"""**Training Function**

This code performs training for a neural network model using the specified hyperparameters, loss function, and optimizer. It trains the model on a training dataset (`train_loader`) and evaluates its performance on a validation dataset (`val_loader`) for a specified number of epochs. The steps are the following:

1. **Training Neural Networks Function**:
   - `def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device="cpu"):`: This function is defined to train the neural network model.
   - `model.to(device)`: It moves the model to the specified computing device, either "cpu" or "cuda" (GPU), to ensure the computations occur on the chosen hardware.

2. **Training Loop**:
   - The code enters a loop that iterates for a specified number of training epochs (default is 10, but it can be overridden with the `epochs` parameter).
   - Inside the loop, training and validation losses are initialized to zero.
   - `model.train()`: The model is set to training mode, which enables features like dropout and batch normalization to behave differently during training.
   - The code then iterates through the training data using the `train_loader`. For each batch:
     - Gradients are cleared using `optimizer.zero_grad()` to prepare for backpropagation.
     - Inputs and targets are moved to the specified computing device.
     - The model makes predictions for the inputs (`output = model(inputs)`) and computes the loss between the predictions and the actual targets.
     - Backpropagation is performed (`loss.backward()`) to compute gradients.
     - The optimizer updates the model's parameters based on the computed gradients (`optimizer.step()`).
     - Training loss is updated based on the batch's loss, and the loss is normalized by the batch size.
   
3. **Validation**:
   - After each epoch, the code performs a validation step:
   - `with torch.no_grad()`: The code enters a block where no gradients are computed, as we're not updating the model.
   - `model.eval()`: The model is set to evaluation mode, which disables dropout and batch normalization changes introduced during training.
   - The validation dataset is iterated using `val_loader`, and for each batch:
     - Inputs and targets are moved to the specified computing device.
     - The model makes predictions.
     - Loss is computed.
     - Validation loss is updated and normalized by the batch size.
     - Correct predictions are counted to calculate the number of correct predictions.
     - The number of total examples is updated.
   
4. **Printing Training Progress**:
   - After each epoch, the code prints information about the training progress, including the current epoch, training loss, validation loss, and accuracy.

5. **Return Loss Statistics**:
   - The function returns a dictionary (`loss_stats`) containing training and validation loss statistics for each epoch. This information can be useful for monitoring and visualizing the model's training progress.

"""

# Training Neural Networks
def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=10, device="cpu"):
    model.to(device)
    loss_stats = {'train': [], "val": []}

    for epoch in range(1, epochs+1):
        training_loss = 0.0
        valid_loss = 0.0
        model.train()  # Set the model in training mode
        for batch in train_loader:
            optimizer.zero_grad()  # Clear gradients for the next batch
            inputs, targets = batch
            inputs = inputs.to(device)
            targets = targets.to(device)
            output = model(inputs)
            loss = loss_fn(output, targets)
            loss.backward()  # Backpropagation to compute gradients
            optimizer.step()  # Apply gradients
            training_loss += loss.data.item() * inputs.size(0)

        training_loss /= len(train_loader.dataset)

        # Validation
        with torch.no_grad():
            model.eval()  # Set the model in evaluation mode
            num_correct = 0
            num_examples = 0
            for batch in val_loader:
                inputs, targets = batch
                inputs = inputs.to(device)
                output = model(inputs)
                targets = targets.to(device)
                loss = loss_fn(output, targets)
                valid_loss += loss.data.item() * inputs.size(0)
                correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)
                num_correct += torch.sum(correct).item()
                num_examples += correct.shape[0]

            valid_loss /= len(val_loader.dataset)
            loss_stats['train'].append(training_loss / len(train_loader))
            loss_stats['val'].append(valid_loss / len(val_loader))

        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch, training_loss,
                                                                                                  valid_loss, num_correct / num_examples))

    return loss_stats

"""**CUDA if available**

This code first checks if a GPU (CUDA) is available on the system. If a GPU is available, it sets the device to use the GPU for model computations; otherwise, it defaults to using the CPU.

Then, it moves the neural network model to the selected device, ensuring that the model's operations are performed on the chosen hardware for training or inference.

"""

# Check if a GPU (mps) is available - GPU(cuda) for NVIDIA 
if torch.backends.mps.is_available():
    device = torch.device('mps')  # Use the GPU  # Use the GPU
    
else:
    device = torch.device('cpu')  # Use the CPU

# Move the model to the selected device
model.to(device)
print(device)

"""**Hyperparameters and Training**:

We set the following Hyperparameters:

   - `learning_rate`: This hyperparameter determines the step size for updating model parameters during optimization. It controls the rate at which the model learns.
   - `epochs`: The number of training epochs, which defines how many times the entire training dataset is processed by the model during training.

We need to define the loss function and optimizer function:

1. **Loss Function**:
   - `loss_fn = torch.nn.CrossEntropyLoss()`: This line defines the loss function that measures the error between the model's predictions and the actual target labels. `CrossEntropyLoss` is commonly used for classification tasks.

2. **Optimizer**:
   - `optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)`: This initializes the optimizer, which is responsible for updating the model's parameters to minimize the loss. In this case, it's Stochastic Gradient Descent (SGD), a popular optimization algorithm.
   - Alternatively, you can use the Adam optimizer by uncommenting the line: `optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)`. Adam is another optimization algorithm.

The last line of code initiates the training process by calling the train function, passing in the model to be trained, the optimizer for updating model parameters, the loss function for calculating errors, the training data loader (train_dl) for input data, the test data loader (test_dl) for validation, and the number of training epochs (epochs). The code records and stores the loss statistics as a result of the training process.

"""

# Hyperparameters
learning_rate = 0.001
epochs = 8

# Loss Function
loss_fn = torch.nn.CrossEntropyLoss()

# Optimizer
#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
# Alternatively, the Adam optimizer can be used:
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Train the model and record loss statistics
loss_stats = train(model, optimizer, loss_fn, train_dl, test_dl, epochs=epochs, device=device)

"""**Visualizing Training and Validation Loss Over Epochs**

This section performs two main tasks:

1) `pd.DataFrame.from_dict(loss_stats)`: This part creates a Pandas DataFrame from the `loss_stats` dictionary. The `loss_stats` dictionary contains training and validation loss values for different epochs.

`.reset_index()`: After creating the DataFrame, the `reset_index` method is called. This operation resets the index of the DataFrame to default integer indices. It's used to convert the dictionary's keys (which represent the epoch numbers) into a proper DataFrame column.

`.melt(id_vars=['index'])`: The `melt` function is used to reshape the DataFrame. In this specific case, it transforms the DataFrame from a wide format to a long format. The `id_vars` parameter specifies which columns to keep as identifiers, and the other columns are combined into two new columns: one for variable names (often used for legend or labels) and another for the corresponding values.


2) It then generates a line plot using Seaborn to display the training and validation loss over different epochs. The plot provides a visual representation of how the loss changes during the training process.

"""

def plot_loss_statistics(loss_stats, title=''):
    # Create a DataFrame from the loss statistics, reset the index, and melt the data
    train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={"index":"epochs"})

    # Create a line plot to visualize the training and validation loss over epochs
    plt.figure(figsize=(10, 6))
    sns.lineplot(data=train_val_loss_df, x="epochs", y="value", hue="variable").set_title('Train-Val Loss/Epoch - '+ title)

plot_loss_statistics(loss_stats, title = 'Fully Connected Network')

"""**Overfitting a Batch**

This code snippet demonstrates the process of overfitting a single batch of data using a neural network model.

 It performs the following steps:

- Setting Training Epochs: It sets the number of training epochs to 300, indicating how many times the model will be updated.

- Loss Function: It defines the loss function to measure the error between model predictions and actual targets.

- Getting a Batch: It obtains a single batch of data from the training dataloader. This batch includes input data and their corresponding target labels.

- Training Loop: It enters a training loop to overfit the provided batch.

In each epoch, the code performs the following:

- Forward pass: Computes model predictions and loss.

- Backpropagation: Computes gradients and updates model parameters.

- Calculates and prints the loss and accuracy for the batch.

Perfect Accuracy Check: It checks for perfect accuracy (optional) and exits the training loop if achieved.

"""

def train_single_batch(model, train_dl, loss_fn, optimizer, device, epochs=300):

    model.to(device)
    model.train()

    for batch in train_dl:
       inputs, targets = batch
       inputs = inputs.to(device)
       targets = targets.to(device)
       break

    # Training loop for overfitting a single batch
    for epoch in range(1, epochs+1):
       # Forward pass
       pred = model(inputs)
       loss = loss_fn(pred, targets)

       # Backpropagation
       optimizer.zero_grad()
       loss.backward()
       optimizer.step()

       # Calculate loss and accuracy
       loss, current = loss.item(), len(inputs)
       print(f"Epoch: {epoch}, Loss: {loss:.6f}")
       acc = (pred.argmax(1) == targets).type(torch.float).mean()
       print(f"Accuracy: {100*acc:.2f}%")

       # Check for perfect accuracy (optional)
       if acc == 1:
        break


train_single_batch(model, train_dl, loss_fn, optimizer, device)

"""**Visualizing Model Predictions on Test Data**

This code snippet creates a visualization of the model's predictions on the test data.

It performs the following actions:

- Create a Figure: It creates a figure for plotting, setting the number of columns and rows to 3x3.

- Evaluation Mode: The model is set to evaluation mode (model.eval()) as no weight updates are required during visualization. This mode ensures consistent behavior, especially for layers like dropout and batch normalization.

- Visualization Loop: It loops through the test data and creates subplots for visualizing the input images. For each subplot, it displays the image, the true label, and some prediction details.

- Prediction Details: The code prints the true label, the associated class name, the scores or probabilities for each class provided by the model, the maximum score, and the index of the class with the maximum score.

- Display Figure: The figure is displayed, showing the model's predictions on the test data along with the original images and labels.

"""

# Title: Visualizing Model Predictions on Test Data

# Create a figure for plotting
figure = plt.figure(figsize=(8, 8))
cols, rows = 3, 3
model.to(device)

# Set the model to evaluation mode as no weight updates are needed during visualization
with torch.no_grad():

    model.eval()
    for batch in test_dl:
        inputs, targets = batch
        inputs_gpu = inputs.to(device)
        targets = targets.to(device)
        device_cpu = torch.device('cpu')
        inputs_cpu = inputs.to(device_cpu)
        print(inputs_cpu.shape)
        break
    # Loop through test data for visualization
    for i in range(1, cols * rows + 1):
        print(inputs_cpu[i].view(28,28).shape)
        figure.add_subplot(rows, cols, i)
        plt.imshow(inputs_cpu[i].view(28,28), cmap="gray")
        plt.title(str(targets[i]) + " " + str(labels_names[targets[i]]))

        # Print prediction details
        print(inputs_gpu[i].shape)
        print()
        print(str(targets[i]) + " " + labels_names[targets[i]])
        scores = model(inputs_gpu[i].float())
        print("All the scores: {}".format(scores))
        print("The max score: {}".format(scores.max()))
        print("The index of the max score: {}".format(scores.argmax()))

# Display the figure with visualized predictions
plt.show()

"""**Learning Rate Finder**

This section defines a function that helps find an appropriate learning rate for training a neural network. It performs the following steps:

- Initialization: It calculates the number of batches in an epoch and initializes the learning rate, losses, and other variables.

- Training Loop: The function loops through the training data. For each batch, it computes the model's predictions, loss, and performs backpropagation to optimize the model.

- Learning Rate Adjustment: The learning rate is adjusted using a specified range and update step. It records the loss and learning rate at each step.

- Exploding Loss Check: It checks for an exploding loss, which may indicate that the learning rate is too high, and returns the results if such a condition is met.

- Best Loss Tracking: It keeps track of the best loss observed during the learning rate search.

- Smoothing Data: The initial and final 5% of data are removed to smooth the graph.

- Returning Results: Finally, the function returns the learning rates and corresponding losses, which can be used to visualize the loss versus learning rate curve to determine an appropriate learning rate for training the model.

"""

number_in_epoch = len(train_dl) - 1
init_value = 1e-8
final_value = 1
print((final_value / init_value) ** (1 / number_in_epoch))

def find_lr(model, loss_fn, optimizer, train_loader, init_value = 1e-8, final_value = 1, device = "cpu"):


    # Calculate the number of batches in an epoch
    number_in_epoch = len(train_loader) - 1

    # Calculate the update step for learning rate
    update_step = (final_value / init_value) ** (1 / number_in_epoch)

    # Initialize learning rate and set it in the optimizer
    lr = init_value
    optimizer.param_groups[0]["lr"] = lr

    # Initialize variables to track loss and best loss
    training_loss = 0.0
    best_loss = 0.0
    best_lr = lr

    # Initialize batch number, losses, and learning rates lists
    batch_num = 0
    losses = []
    log_lrs = []

    # Set the model in training mode
    model.train()

    # Loop through the training data
    for data in train_loader:
        batch_num += 1
        inputs, targets = data
        inputs = inputs.to(device)
        targets = targets.to(device)

        # Clear gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)

        # Calculate loss
        loss = loss_fn(outputs, targets)

        # Check for exploding loss
        if batch_num > 1 and loss > 10 * best_loss:
            print('Exploded current loss: {} best_loss: {}'.format(loss, best_loss))
            print(f'Best learning rate: {best_lr}')
            return log_lrs, losses, best_lr, best_loss.item()

        # Record the best loss
        if loss < best_loss or batch_num == 1:
            best_loss = loss
            best_lr = lr

        # Store loss and learning rate values
        losses.append(loss.item())
        log_lrs.append(lr)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        # Update learning rate for the next step and set it in the optimizer
        lr *= update_step
        optimizer.param_groups[0]["lr"] = lr

    return log_lrs, losses, best_lr, best_loss.item()

"""**Apply Learning Rate Finder and visualize Learning Rate versus Loss**

After running the first line of code, we get the best learning rate variables and the values of lrs and losses generated to find it.

The plot represents the relationship between learning rates and corresponding losses. The x-axis is set to a logarithmic scale using plt.xscale("log"). This allows for a better representation of a wide range of learning rates.


"""
model_empty = NeuralNetwork()
model_empty.to(device)
# Find learning rates
log_lrs, losses, best_lr, best_loss = find_lr(model_empty, loss_fn, optimizer, train_dl, device=device)
# Print the learning rates
#print(lrs)

def plot_learning_rates(lrs, losses, best_lr, best_loss, title):
    plt.plot(lrs, losses)
    plt.xscale("log")
    plt.xlabel("Learning rate")
    plt.ylabel("Loss")
    plt.title("Learning Rate versus Loss - " + title)
    plt.scatter(best_lr, best_loss, color='red', marker='o', label=f'Best LR: {best_lr}')
    plt.show()

# Title: Visualizing Learning Rate versus Loss
plot_learning_rates(log_lrs, losses, best_lr, best_loss, title = 'Fully Connected Network')

"""#Convolutional Neural Network (CNN)

Convolution parameters (Conv2D):
- ```in_channel``` is the number of input channels (1 for grey images, 3 for rgb)
- ```out_channel``` the number of output channels
- ```kernel_size``` eight and width of the kernel filter, if only one value is passed then eight = width
- ```stride``` how many steps across the input we move when we adjust the filter to a new position
- ```padding``` it can happen that we don't have enough elements in our input to do a full convolution. In these cases, the empty values are filled with the ```padding``` values; Not useful padding in this framework

**Convolutional Neural Network (CNN) Architecture**

This code defines a convolutional neural network (CNN) architecture for image classification. It consists of convolutional layers, max-pooling layers, and fully connected layers. The CNN architecture is designed to process and classify image data. The forward method performs the following steps:

- Set 1: Applies the first convolution layer, ReLU activation, and max-pooling to the input image.

- Set 2: Applies the second convolution layer, ReLU activation, and another max-pooling operation.

- Flatten: Flattens the feature maps to a 1D vector to prepare for fully connected layers.

- Fully Connected Layers: Utilizes fully connected layers with ReLU activations to make final predictions.

"""

# Title: Convolutional Neural Network (CNN) Architecture

class CNN(torch.nn.Module):
    def __init__(self):
        super(CNN, self).__init__()

        # Convolution Layer 1
        self.cnn1 = torch.nn.Conv2d(in_channels=1, out_channels=64,
                              kernel_size=6, stride=1, padding=0)
        self.relu1 = torch.nn.ReLU()

        # Max Pooling Layer 1
        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=3, stride=2)

        # Convolution Layer 2
        self.cnn2 = torch.nn.Conv2d(in_channels=64, out_channels=128,
                              kernel_size=3, stride=2, padding=3)
        self.relu2 = torch.nn.ReLU()

        # Max Pooling Layer 2
        self.maxpool2 = torch.nn.MaxPool2d(kernel_size=2, stride=1)

        # Fully Connected Layers
        self.dropout = torch.nn.Dropout()
        self.fc1 = torch.nn.Linear(128 * 7 * 7, 256)
        self.relu3 = torch.nn.ReLU()
        self.dropout2 = torch.nn.Dropout()
        self.fc2 = torch.nn.Linear(256, 128)
        self.relu4 = torch.nn.ReLU()
        self.fc3 = torch.nn.Linear(128, 10)

    def forward(self, x, to_print=False):
        # Set 1
        if to_print:
            print('INPUT', x.shape)
        x = self.cnn1(x)
        if to_print:
            print('CNN1', x.shape)
        x = self.relu1(x)
        x = self.maxpool1(x)
        if to_print:
            print('MAXPOOL1', x.shape)

        # Set 2
        x = self.cnn2(x)
        if to_print:
            print('CNN2', x.shape)
        x = self.relu2(x)
        x = self.maxpool2(x)
        if to_print:
            print("after the 2nd maxpool: {}".format(x.shape))

        # Flatten
        x = x.view(x.size(0), -1)
        if to_print:
            print("after the flatten: {}".format(x.shape))
        #x = self.dropout(x)
        x = self.fc1(x)
        x = self.relu3(x)
        if to_print:
            print('FINAL', x.shape)
        x = self.dropout2(x)
        x = self.fc2(x)
        x = self.relu4(x)
        x = self.fc3(x)  # Get the logits

        return x

model_CNN = CNN()  # Create an instance of the NeuralNetwork
print(model_CNN)  # Display the model architecture

# To view the model's state dictionary
# state_dict = model_CNN.state_dict()
# print(state_dict)

"""**Modified Training Function for Neural Networks**

The train2 function is designed to train a neural network model with specified parameters, including the number of training epochs, the training data loader, and the validation data loader. It also allows for optional printing of intermediate results for debugging or analysis.

The function performs the following actions:

- Move Model to Device: It moves the neural network model to the specified device (e.g., CPU or GPU).

- Epoch Iteration: It iterates through the specified number of epochs, performing training and validation at each epoch.

- Training Loop: Within each epoch, it iterates through the training data batches, performs forward and backward passes, and updates model weights using backpropagation.

- Validation: After each epoch, it evaluates the model's performance on the validation dataset and calculates the accuracy.

- Printing Results: If the to_print parameter is set to True, it prints detailed results for the first batch and breaks out of the loop, allowing debugging and analysis.

"""

## few changes to the train function used above.

# Title: Modified Training Function for Neural Networks

def train2(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device="cpu", to_print=False):
    # Move the model to the specified device
    model.to(device)
    loss_stats_CNN = {'train': [], "val": []}

    # Iterate through epochs
    for epoch in range(1, epochs+1):
        training_loss = 0.0
        valid_loss = 0.0
        model.train()  # Set the model in training mode

        # Iterate through training batches
        for batch in train_loader:
            optimizer.zero_grad()  # Clear gradients for the next training step
            inputs, targets = batch
            inputs = inputs.to(device)
            targets = targets.to(device)

            # Forward pass
            output = model(inputs, to_print)

            # Calculate loss
            loss = loss_fn(output, targets)

            # Backpropagation and optimization
            loss.backward()  # Compute gradients
            optimizer.step()  # Apply gradients

            training_loss += loss.data.item() * inputs.size(0)

            if to_print:
                break

        training_loss /= len(train_loader.dataset)

        if to_print:
            break

        with torch.no_grad():
            model.eval()  # Set the model in evaluation mode
            num_correct = 0
            num_examples = 0

            # Iterate through validation batches
            for batch in val_loader:
                inputs, targets = batch
                inputs = inputs.to(device)
                output = model(inputs)
                targets = targets.to(device)
                loss = loss_fn(output, targets)
                valid_loss += loss.data.item() * inputs.size(0)

                # Calculate the number of correct predictions
                correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)
                num_correct += torch.sum(correct).item()
                num_examples += correct.shape[0]

            valid_loss /= len(val_loader.dataset)
            loss_stats_CNN['train'].append(training_loss / len(train_loader))
            loss_stats_CNN['val'].append(valid_loss / len(val_loader))

        # Print epoch summary
        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch, training_loss, valid_loss, num_correct / num_examples))

    return loss_stats_CNN

"""**Data Reshaping and DataLoader Initialization**

This section prepares the training and test datasets by reshaping and creating DataLoader objects.

"""

# Define the batch size
batch_size = 64

# Reshape training data and create a training and test dataset and dataloader
x_train_tensor_rs = x_train_tensor.reshape(-1, 1, 28, 28)
x_test_tensor_rs = x_test_tensor.reshape(-1, 1, 28, 28)
train_ds_CNN, train_dl_CNN, test_ds_CNN, test_dl_CNN = create_data_loaders(x_train_tensor_rs, y_train_tens, x_test_tensor_rs, y_test_tens, batch_size = batch_size)

# Print the shapes of the first batch of data and labels
for x, y in train_dl_CNN:
    print("Data Shape:", x.shape, "Label Shape:", y.shape)
    break

# Additional prints for data exploration
print("Shape of the reshaped training data:", x_train_tensor_rs.shape)
#print("Example of a reshaped data sample:", x_train_tensor_rs[2])
print("Labels for the training data:", y_train_tens)

# Check if a GPU (CUDA) is available - GPU(cuda) for NVIDIA 
if torch.backends.mps.is_available():
    device = torch.device('mps')  # Use the GPU  # Use the GPU
else:
    device = torch.device('cpu')  # Use the CPU

# Move the model to the selected device
model_CNN.to(device)

"""**Training**"""

# Hyperparameters
learning_rate = 0.0005
epochs = 20

# Loss Function
loss_fn_CNN = torch.nn.CrossEntropyLoss()

# Optimizer
#optimizer_CNN = torch.optim.SGD(model_CNN.parameters(), lr=learning_rate)
# Alternatively, the Adam optimizer can be used:
optimizer_CNN = torch.optim.Adam(model_CNN.parameters(), lr=learning_rate)


loss_stats_CNN = train2(model_CNN, optimizer_CNN, loss_fn_CNN, train_dl_CNN, test_dl_CNN, epochs=epochs, device = device)
#model_CNN.cpu().relu2.weight.shape

"""**Overfit of a Batch**"""

train_single_batch(model_CNN, train_dl_CNN, loss_fn_CNN, optimizer_CNN, device)

"""**Loss: Train vs Validation**"""

plot_loss_statistics(loss_stats_CNN, title = 'Convolutional Network')

"""**Best Learning Rate**"""
model_CNN_empty = CNN()
model_CNN_empty.to(device)
# Find learning rates
log_lrs_CNN, losses_CNN, best_lr_CNN, best_loss_CNN = find_lr(model_CNN_empty, loss_fn_CNN, optimizer_CNN, train_dl_CNN, device=device)
# Print the learning rates
#print(lrs)

# Title: Visualizing Learning Rate versus Loss
plt.figure()
plot_learning_rates(log_lrs_CNN, losses_CNN, best_lr_CNN, best_loss_CNN, title = 'Convolutional Network')

def calculate_class_accuracy(model, test_loader, device):
    class_correct = np.zeros(10)
    total_correct = np.zeros(10)
    model.eval()
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            predicted = torch.max(outputs, 1)[1]
            c = (predicted == labels).squeeze()

            for i in range(len(labels)):
                label = labels[i]
                class_correct[label] += c[i].item()
                total_correct[label] += 1

    class_accuracies = [class_correct[i] * 100 / total_correct[i] if total_correct[i] > 0 else 0.0 for i in range(10)]

    return class_accuracies

# Usage:
class_accuracies = calculate_class_accuracy(model, test_dl, device)
for i in range(10):
    print("Accuracy of {}: {:.2f}%".format(labels_names[i], class_accuracies[i]))

# Usage:
class_accuracies = calculate_class_accuracy(model_CNN, test_dl_CNN, device)
for i in range(10):
    print("Accuracy of {}: {:.2f}%".format(labels_names[i], class_accuracies[i]))